{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Started with Jupyter on Google Cloud\n",
    "In the following you find various helpful code examples which show you how to access data or start ML routines on Google Cloud resources like CPUs, GPUs, or TPUs.\n",
    "\n",
    "Our first task is to import all necessary libraries used in the examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access Data on Google Cloud Storage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cloud Storage is a storage service in the Google Cloud. It can store virtually infinite amounts of data. Typically, Cloud Storage is used to store files with unstructured data, such as images, text files, and semi-structured file formats, such as CSV, Avro, Parquet, and TFRecords.\n",
    "\n",
    "We start by creating a Cloud Storage client in Python. The client allows us to interact with the Cloud Storage service. With the client we can download and upload files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "client = storage.Client()\n",
    "print(\"Client created using default project: {}\".format(client.project))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explicitly specify a project when constructing the client, set the `project` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = storage.Client(project='your-project-id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we work with a bucket which is a root folder in Cloud Storage. Buckets can contain many files and have (practically) no size limit. Here is how we access our bucket for the hackathon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"ecb-fsf-hackathon-base-data\"\n",
    "bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "print(\"Bucket name: {}\".format(bucket.name))\n",
    "print(\"Bucket location: {}\".format(bucket.location))\n",
    "print(\"Bucket storage class: {}\".format(bucket.storage_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's list all files in the bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs = bucket.list_blobs()\n",
    "\n",
    "print(\"Blobs in {}:\".format(bucket.name))\n",
    "for item in blobs:\n",
    "    print(\"\\t\" + item.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the gsutil command line tool for a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls gs://{bucket_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get details about one of the files, download it, and load into a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_name = \"sample.csv\"\n",
    "blob = bucket.get_blob(blob_name)\n",
    "\n",
    "print(\"Name: {}\".format(blob.id))\n",
    "print(\"Size: {} bytes\".format(blob.size))\n",
    "print(\"Content type: {}\".format(blob.content_type))\n",
    "print(\"Public URL: {}\".format(blob.public_url))\n",
    "\n",
    "output_file_name = \"/tmp/sample.csv\"\n",
    "blob.download_to_filename(output_file_name)\n",
    "\n",
    "print(\"Downloaded blob {} to {}.\".format(blob.name, output_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the same can be achieved using the gsutil command line tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp gs://{bucket_name}/{blob_name} /tmp/{blob_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the file stored locally, we can load it into a Pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(output_file_name, header=None)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we should have a look into the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use Panda's built-in support for Google Cloud Storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('gs://ecb-fsf-hackathon-base-data/sample.csv', header=None)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And .head() should return the same lines as with our manual download:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learn more about interacting with Cloud Storage in the following tutorials:**\n",
    "- [Cloud Storage client library](../tutorials/storage/Cloud%20Storage%20client%20library.ipynb)\n",
    "- [Storage command-line tool](../tutorials/storage/Storage%20command-line%20tool.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access Tables & Views on Google BigQuery\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client creating using default project: ecb-fsf-hackathon-base\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "client = bigquery.Client(location=\"EU\")\n",
    "print(\"Client creating using default project: {}\".format(client.project))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explicitly specify a project when constructing the client, set the `project` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = bigquery.Client(location=\"US\", project=\"your-project-id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>url</th>\n",
       "      <th>product_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>29</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Wein, Spirituosen &amp; Tabak Fruchtwein &amp; Weinmis...</td>\n",
       "      <td>https://shop.rewe.de/p/kunzmann-bio-gluehwein-...</td>\n",
       "      <td>Die Weinmacher Deidesheimer Hofstück Portugies...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 category  \\\n",
       "count                                                  60   \n",
       "unique                                                 29   \n",
       "top     Wein, Spirituosen & Tabak Fruchtwein & Weinmis...   \n",
       "freq                                                   12   \n",
       "\n",
       "                                                      url  \\\n",
       "count                                                  60   \n",
       "unique                                                 60   \n",
       "top     https://shop.rewe.de/p/kunzmann-bio-gluehwein-...   \n",
       "freq                                                    1   \n",
       "\n",
       "                                             product_name  \n",
       "count                                                  60  \n",
       "unique                                                 60  \n",
       "top     Die Weinmacher Deidesheimer Hofstück Portugies...  \n",
       "freq                                                    1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    SELECT category, url, product_name\n",
    "    FROM `ecb-fsf-hackathon-base.hackathon_dataset.web_scraped_data`\n",
    "    LIMIT 60\n",
    "\"\"\"\n",
    "query_job = client.query(query, location=\"EU\")\n",
    "df = query_job.to_dataframe()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>url</th>\n",
       "      <th>product_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wein, Spirituosen &amp; Tabak Spirituosen &amp; -misch...</td>\n",
       "      <td>https://shop.rewe.de/p/ramazzotti-amaro-1l/138...</td>\n",
       "      <td>Ramazzotti Amaro 1l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wein, Spirituosen &amp; Tabak Spirituosen &amp; -misch...</td>\n",
       "      <td>https://shop.rewe.de/p/aperol-aperitivo-italia...</td>\n",
       "      <td>Aperol Aperitivo Italiano 1l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wein, Spirituosen &amp; Tabak Fruchtwein &amp; Weinmis...</td>\n",
       "      <td>https://shop.rewe.de/p/kunzmann-bio-gluehwein-...</td>\n",
       "      <td>Kunzmann Bio Glühwein weiß 1l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wein, Spirituosen &amp; Tabak Fruchtwein &amp; Weinmis...</td>\n",
       "      <td>https://shop.rewe.de/p/gerstacker-apfelpunsch-...</td>\n",
       "      <td>Gerstacker Apfelpunsch 1l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wein, Spirituosen &amp; Tabak Spirituosen &amp; -misch...</td>\n",
       "      <td>https://shop.rewe.de/p/loerch-williams-christ-...</td>\n",
       "      <td>Lörch Williams Christ Birne 1l</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            category  \\\n",
       "0  Wein, Spirituosen & Tabak Spirituosen & -misch...   \n",
       "1  Wein, Spirituosen & Tabak Spirituosen & -misch...   \n",
       "2  Wein, Spirituosen & Tabak Fruchtwein & Weinmis...   \n",
       "3  Wein, Spirituosen & Tabak Fruchtwein & Weinmis...   \n",
       "4  Wein, Spirituosen & Tabak Spirituosen & -misch...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://shop.rewe.de/p/ramazzotti-amaro-1l/138...   \n",
       "1  https://shop.rewe.de/p/aperol-aperitivo-italia...   \n",
       "2  https://shop.rewe.de/p/kunzmann-bio-gluehwein-...   \n",
       "3  https://shop.rewe.de/p/gerstacker-apfelpunsch-...   \n",
       "4  https://shop.rewe.de/p/loerch-williams-christ-...   \n",
       "\n",
       "                     product_name  \n",
       "0             Ramazzotti Amaro 1l  \n",
       "1    Aperol Aperitivo Italiano 1l  \n",
       "2   Kunzmann Bio Glühwein weiß 1l  \n",
       "3       Gerstacker Apfelpunsch 1l  \n",
       "4  Lörch Williams Christ Birne 1l  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also execute a query using the BigQuery magic expression in a cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query with job ID: ba9b9c6f-18b5-4cbd-8ac9-f1b8db501321\n",
      "Query executing: 0.40s\n",
      "Query complete after 0.64s\n"
     ]
    }
   ],
   "source": [
    "%%bigquery --verbose df\n",
    "SELECT category, Count(*) as Occurence\n",
    "FROM `ecb-fsf-hackathon-base.hackathon_dataset.web_scraped_data`\n",
    "GROUP BY category\n",
    "ORDER BY Occurence DESC\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>Occurence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lebensmittel Frühstück</td>\n",
       "      <td>590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Frische &amp; Kühlung Joghurt, Pudding &amp; Milchsnac...</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lebensmittel Backzutaten</td>\n",
       "      <td>390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lebensmittel Schokolade/Riegel</td>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lebensmittel Fertiggerichte</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            category  Occurence\n",
       "0                             Lebensmittel Frühstück        590\n",
       "1  Frische & Kühlung Joghurt, Pudding & Milchsnac...        448\n",
       "2                           Lebensmittel Backzutaten        390\n",
       "3                     Lebensmittel Schokolade/Riegel        372\n",
       "4                        Lebensmittel Fertiggerichte        365"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learn more about interacting with BigQuery in the following tutorials:**\n",
    "- [BigQuery basics](../tutorials/bigquery/BigQuery%20basics.ipynb)\n",
    "- [BigQuery command-line tool](../tutorials/bigquery/BigQuery%20command-line%20tool.ipynb)\n",
    "- [BigQuery query magic](../tutorials/bigquery/BigQuery%20query%20magic.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud AI APIs and Cloud AutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful resources to get started with our Cloud APIs for NLP and [AutoML](https://cloud.google.com/automl/) for NLP:\n",
    "* [Cloud NLP Intro](https://cloud.google.com/natural-language/)\n",
    "* [Cloud Natural Language API Docs](https://cloud.google.com/natural-language/docs/)\n",
    "* [Cloud AutoML Get Started Guides](https://cloud.google.com/natural-language/overview/docs/get-started)\n",
    "* [Cloud AutoML NLP in the Console](https://console.cloud.google.com/natural-language)\n",
    "\n",
    "There is also [Cloud AutoML Tables](https://cloud.google.com/automl-tables/) to build ML models on tabular data (e.g. from BigQuery):\n",
    "* [Cloud AutoML Tables Intro](https://cloud.google.com/automl-tables/)\n",
    "* [Cloud AutoML Tables Docs](https://cloud.google.com/automl-tables/docs/)\n",
    "* [Cloud AutoML Tables in the Console](https://console.cloud.google.com/automl-tables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation with Apache Beam (and Cloud Dataflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install apache-beam[gcp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "pipeline_options = PipelineOptions.from_dictionary({\n",
    "    'runner': 'DirectRunner',\n",
    "# Run it massively parallel on Dataflow with\n",
    "#   'runner': 'DataflowRunner'\n",
    "    'job_name': 'notebook',\n",
    "    'streaming': True\n",
    "})\n",
    "\n",
    "def collect(i):\n",
    "    output.append(i)\n",
    "    return True\n",
    "\n",
    "output = []\n",
    "\n",
    "p = beam.Pipeline(options=pipeline_options)\n",
    "\n",
    "pipeline = (\n",
    "    p \n",
    "    | 'generate' >> beam.Create(range(1000))\n",
    "    | 'square' >> beam.Map(lambda x: x**2)\n",
    "    | \"print\" >> beam.Map(collect)\n",
    ")\n",
    "\n",
    "result = p.run()\n",
    "result.wait_until_finish()\n",
    "\n",
    "output[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models with Google Cloud AI Platform Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to enable the ML and Container Registry APIs in our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud services enable ml.googleapis.com\n",
    "!gcloud services enable containerregistry.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to create a bucket for the staging and training results. Replace with your favorite name (needs to be globally unique!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil mb gs://[YOUR_GCS_BUCKET]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ready to start our Training Job! Fill in in your bucket name where you find brackets. You can modify the model_dir parameter to change where the training output is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud ml-engine jobs submit training $JOB_NAME \\\n",
    "    --staging-bucket [YOUR_GCS_BUCKET] \\\n",
    "    --runtime-version 1.8 \\\n",
    "    --scale-tier BASIC_TPU \\\n",
    "    --module-name resnet.resnet_main \\\n",
    "    --package-path resnet/ \\\n",
    "    --region us-central1 \\\n",
    "    -- \\\n",
    "    --data_dir=gs://cloud-tpu-test-datasets/fake_imagenet \\\n",
    "    --model_dir=gs://[YOUR_GCS_BUCKET]/training_result/ \\\n",
    "    --resnet_depth=50 \\\n",
    "    --train_steps=1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn more about AI Platform Training & Serving with ML Engine:\n",
    "- [Training & Serving on ML Engine with SciKit Learn](../tutorials/cloud-ml-engine/Training%20and%20prediction%20with%20scikit-learn.ipynb)\n",
    "- [Github Repo full of Training & Prediction Examples](https://github.com/GoogleCloudPlatform/cloudml-samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visit the notebook [evaluation.ipynb](./evaluation.ipynb).**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
