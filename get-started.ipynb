{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Started with Jupyter on Google Cloud\n",
    "In the following you find various helpful code examples which show you how to access data or start ML routines on Google Cloud resources like CPUs, GPUs, or TPUs.\n",
    "\n",
    "Our first task is to import all necessary libraries used in the examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access Data on Google Cloud Storage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cloud Storage is a storage service in the Google Cloud. It can store virtually infinite amounts of data. Typically, Cloud Storage is used to store files with unstructured data, such as images, text files, and semi-structured file formats, such as CSV, Avro, Parquet, and TFRecords.\n",
    "\n",
    "We start by creating a Cloud Storage client in Python. The client allows us to interact with the Cloud Storage service. With the client we can download and upload files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "client = storage.Client()\n",
    "print(\"Client created using default project: {}\".format(client.project))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explicitly specify a project when constructing the client, set the `project` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = storage.Client(project='your-project-id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we work with a bucket which is a root folder in Cloud Storage. Buckets can contain many files and have (practically) no size limit. Here is how we access our bucket for the hackathon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"ecb-fsf-hackathon-base-data\"\n",
    "bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "print(\"Bucket name: {}\".format(bucket.name))\n",
    "print(\"Bucket location: {}\".format(bucket.location))\n",
    "print(\"Bucket storage class: {}\".format(bucket.storage_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's list all files in the bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs = bucket.list_blobs()\n",
    "\n",
    "print(\"Blobs in {}:\".format(bucket.name))\n",
    "for item in blobs:\n",
    "    print(\"\\t\" + item.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the gsutil command line tool for a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls gs://{bucket_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get details about one of the files, download it, and load into a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_name = \"sample.csv\"\n",
    "blob = bucket.get_blob(blob_name)\n",
    "\n",
    "print(\"Name: {}\".format(blob.id))\n",
    "print(\"Size: {} bytes\".format(blob.size))\n",
    "print(\"Content type: {}\".format(blob.content_type))\n",
    "print(\"Public URL: {}\".format(blob.public_url))\n",
    "\n",
    "output_file_name = \"/tmp/sample.csv\"\n",
    "blob.download_to_filename(output_file_name)\n",
    "\n",
    "print(\"Downloaded blob {} to {}.\".format(blob.name, output_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the same can be achieved using the gsutil command line tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp gs://{bucket_name}/{blob_name} /tmp/{blob_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the file stored locally, we can load it into a Pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(output_file_name, header=None)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we should have a look into the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use Panda's built-in support for Google Cloud Storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('gs://ecb-fsf-hackathon-base-data/sample.csv', header=None)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And .head() should return the same lines as with our manual download:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learn more about interacting with Cloud Storage in the following tutorials:**\n",
    "- [Cloud Storage client library](../tutorials/storage/Cloud%20Storage%20client%20library.ipynb)\n",
    "- [Storage command-line tool](../tutorials/storage/Storage%20command-line%20tool.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access Tables & Views on Google BigQuery\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "client = bigquery.Client(location=\"EU\")\n",
    "print(\"Client creating using default project: {}\".format(client.project))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explicitly specify a project when constructing the client, set the `project` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = bigquery.Client(location=\"US\", project=\"your-project-id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    SELECT `Set`, URL, Label\n",
    "    FROM `ecb-fsf-hackathon-base.hackathon_dataset.data_table`\n",
    "    LIMIT 60\n",
    "\"\"\"\n",
    "query_job = client.query(query, location=\"EU\")\n",
    "df = query_job.to_dataframe()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    SELECT category, url, product_name\n",
    "    FROM `ecb-fsf-hackathon-base.hackathon_dataset.food_unique_products_classified`\n",
    "    LIMIT 60\n",
    "\"\"\"\n",
    "query_job = client.query(query, location=\"EU\")\n",
    "df = query_job.to_dataframe()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also execute a query using the BigQuery magic expression in a cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery --verbose df\n",
    "SELECT category, Count(*) as Occurence\n",
    "FROM `ecb-fsf-hackathon-base.hackathon_dataset.food_unique_products_classified`\n",
    "GROUP BY category\n",
    "ORDER BY Occurence DESC\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learn more about interacting with BigQuery in the following tutorials:**\n",
    "- [BigQuery basics](../tutorials/bigquery/BigQuery%20basics.ipynb)\n",
    "- [BigQuery command-line tool](../tutorials/bigquery/BigQuery%20command-line%20tool.ipynb)\n",
    "- [BigQuery query magic](../tutorials/bigquery/BigQuery%20query%20magic.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud AI APIs and Cloud AutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful resources to get started with our Cloud APIs for NLP and [AutoML](https://cloud.google.com/automl/) for NLP:\n",
    "* [Cloud NLP Intro](https://cloud.google.com/natural-language/)\n",
    "* [Cloud Natural Language API Docs](https://cloud.google.com/natural-language/docs/)\n",
    "* [Cloud AutoML Get Started Guides](https://cloud.google.com/natural-language/overview/docs/get-started)\n",
    "* [Cloud AutoML NLP in the Console](https://console.cloud.google.com/natural-language)\n",
    "\n",
    "There is also [Cloud AutoML Tables](https://cloud.google.com/automl-tables/) to build ML models on tabular data (e.g. from BigQuery):\n",
    "* [Cloud AutoML Tables Intro](https://cloud.google.com/automl-tables/)\n",
    "* [Cloud AutoML Tables Docs](https://cloud.google.com/automl-tables/docs/)\n",
    "* [Cloud AutoML Tables in the Console](https://console.cloud.google.com/automl-tables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation with Apache Beam (and Cloud Dataflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install apache-beam[gcp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "pipeline_options = PipelineOptions.from_dictionary({\n",
    "    'runner': 'DirectRunner',\n",
    "# Run it massively parallel on Dataflow with\n",
    "#   'runner': 'DataflowRunner'\n",
    "    'job_name': 'notebook',\n",
    "    'streaming': True\n",
    "})\n",
    "\n",
    "def collect(i):\n",
    "    output.append(i)\n",
    "    return True\n",
    "\n",
    "output = []\n",
    "\n",
    "p = beam.Pipeline(options=pipeline_options)\n",
    "\n",
    "pipeline = (\n",
    "    p \n",
    "    | 'generate' >> beam.Create(range(1000))\n",
    "    | 'square' >> beam.Map(lambda x: x**2)\n",
    "    | \"print\" >> beam.Map(collect)\n",
    ")\n",
    "\n",
    "result = p.run()\n",
    "result.wait_until_finish()\n",
    "\n",
    "output[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models with Google Cloud AI Platform Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to enable the ML and Container Registry APIs in our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud services enable ml.googleapis.com\n",
    "!gcloud services enable containerregistry.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to create a bucket for the staging and training results. Replace with your favorite name (needs to be globally unique!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil mb gs://[YOUR_GCS_BUCKET]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ready to start our Training Job! Fill in in your bucket name where you find brackets. You can modify the model_dir parameter to change where the training output is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud ml-engine jobs submit training $JOB_NAME \\\n",
    "    --staging-bucket [YOUR_GCS_BUCKET] \\\n",
    "    --runtime-version 1.8 \\\n",
    "    --scale-tier BASIC_TPU \\\n",
    "    --module-name resnet.resnet_main \\\n",
    "    --package-path resnet/ \\\n",
    "    --region us-central1 \\\n",
    "    -- \\\n",
    "    --data_dir=gs://cloud-tpu-test-datasets/fake_imagenet \\\n",
    "    --model_dir=gs://[YOUR_GCS_BUCKET]/training_result/ \\\n",
    "    --resnet_depth=50 \\\n",
    "    --train_steps=1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn more about AI Platform Training & Serving with ML Engine:\n",
    "- [Training & Serving on ML Engine with SciKit Learn](../tutorials/cloud-ml-engine/Training%20and%20prediction%20with%20scikit-learn.ipynb)\n",
    "- [Github Repo full of Training & Prediction Examples](https://github.com/GoogleCloudPlatform/cloudml-samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visit the notebook [evaluation.ipynb](./evaluation.ipynb).**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
