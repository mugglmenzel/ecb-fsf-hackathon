{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Started with Jupyter on Google Cloud\n",
    "In the following you find various helpful code examples which show you how to access data or start ML routines on Google Cloud resources like CPUs, GPUs, or TPUs.\n",
    "\n",
    "Our first task is to import all necessary libraries used in the examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access Data on Google Cloud Storage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cloud Storage is a storage service in the Google Cloud. It can store virtually infinite amounts of data. Typically, Cloud Storage is used to store files with unstructured data, such as images, text files, and semi-structured file formats, such as CSV, Avro, Parquet, and TFRecords.\n",
    "\n",
    "We start by creating a Cloud Storage client in Python. The client allows us to interact with the Cloud Storage service. With the client we can download and upload files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "client = storage.Client()\n",
    "print(\"Client created using default project: {}\".format(client.project))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explicitly specify a project when constructing the client, set the `project` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = storage.Client(project='your-project-id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we work with a bucket which is a root folder in Cloud Storage. Buckets can contain many files and have (practically) no size limit. Here is how we access our bucket for the hackathon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"ecb-fsf-hackathon-base-data\"\n",
    "bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "print(\"Bucket name: {}\".format(bucket.name))\n",
    "print(\"Bucket location: {}\".format(bucket.location))\n",
    "print(\"Bucket storage class: {}\".format(bucket.storage_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's list all files in the bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs = bucket.list_blobs()\n",
    "\n",
    "print(\"Blobs in {}:\".format(bucket.name))\n",
    "for item in blobs:\n",
    "    print(\"\\t\" + item.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the gsutil command line tool for a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls gs://{bucket_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get details about one of the files, download it, and load into a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_name = \"sample.csv\"\n",
    "blob = bucket.get_blob(blob_name)\n",
    "\n",
    "print(\"Name: {}\".format(blob.id))\n",
    "print(\"Size: {} bytes\".format(blob.size))\n",
    "print(\"Content type: {}\".format(blob.content_type))\n",
    "print(\"Public URL: {}\".format(blob.public_url))\n",
    "\n",
    "output_file_name = \"/tmp/sample.csv\"\n",
    "blob.download_to_filename(output_file_name)\n",
    "\n",
    "print(\"Downloaded blob {} to {}.\".format(blob.name, output_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the same can be achieved using the gsutil command line tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp gs://{bucket_name}/{blob_name} /tmp/{blob_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the file stored locally, we can load it into a Pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(output_file_name, header=None)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we should have a look into the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use Panda's built-in support for Google Cloud Storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Süßes &amp; Salziges Schokolade Tafelschokolade Vo...</td>\n",
       "      <td>https://shop.rewe.de/p/davert-echter-basmati-r...</td>\n",
       "      <td>Meica Mini Wini Singles extra zart 260g</td>\n",
       "      <td>Teigwaren aus Hartweizengrieß</td>\n",
       "      <td>product_id_store</td>\n",
       "      <td>fa3ff18774b0e1304931e13b31296f65</td>\n",
       "      <td>5kg (1 kg = 0,80 )</td>\n",
       "      <td>0,50</td>\n",
       "      <td>gram</td>\n",
       "      <td>3.99</td>\n",
       "      <td>...</td>\n",
       "      <td>FALSE</td>\n",
       "      <td>score</td>\n",
       "      <td>1</td>\n",
       "      <td>https://shop.rewe.de/p/davert-echter-basmati-r...</td>\n",
       "      <td>Nahrung</td>\n",
       "      <td>01141</td>\n",
       "      <td>0111</td>\n",
       "      <td>011</td>\n",
       "      <td>01</td>\n",
       "      <td>Controversial_Classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0   \\\n",
       "count                                                  13   \n",
       "unique                                                 13   \n",
       "top     Süßes & Salziges Schokolade Tafelschokolade Vo...   \n",
       "freq                                                    1   \n",
       "\n",
       "                                                       1   \\\n",
       "count                                                  13   \n",
       "unique                                                 13   \n",
       "top     https://shop.rewe.de/p/davert-echter-basmati-r...   \n",
       "freq                                                    1   \n",
       "\n",
       "                                             2   \\\n",
       "count                                        13   \n",
       "unique                                       13   \n",
       "top     Meica Mini Wini Singles extra zart 260g   \n",
       "freq                                          1   \n",
       "\n",
       "                                   3                 4   \\\n",
       "count                               9                13   \n",
       "unique                              9                13   \n",
       "top     Teigwaren aus Hartweizengrieß  product_id_store   \n",
       "freq                                1                 1   \n",
       "\n",
       "                                      5                    6     7     8   \\\n",
       "count                                 13                   13    12    13   \n",
       "unique                                13                   13    10     5   \n",
       "top     fa3ff18774b0e1304931e13b31296f65  5kg (1 kg = 0,80 )  0,50  gram   \n",
       "freq                                   1                    1     2     8   \n",
       "\n",
       "          9   ...     14     15  16  \\\n",
       "count     13  ...     13      1  13   \n",
       "unique    11  ...      2      1   2   \n",
       "top     3.99  ...  FALSE  score   1   \n",
       "freq       2  ...     12      1  12   \n",
       "\n",
       "                                                       17       18     19  \\\n",
       "count                                                  13       13     13   \n",
       "unique                                                 13        7     13   \n",
       "top     https://shop.rewe.de/p/davert-echter-basmati-r...  Nahrung  01141   \n",
       "freq                                                    1        4      1   \n",
       "\n",
       "          20   21  22                            23  \n",
       "count     13   13  13                             1  \n",
       "unique     8    3   3                             1  \n",
       "top     0111  011  01  Controversial_Classification  \n",
       "freq       4   10  10                             1  \n",
       "\n",
       "[4 rows x 24 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('gs://ecb-fsf-hackathon-base-data/sample.csv', header=None)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And .head() should return the same lines as with our manual download:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learn more about interacting with Cloud Storage in the following tutorials:**\n",
    "- [Cloud Storage client library](../tutorials/storage/Cloud%20Storage%20client%20library.ipynb)\n",
    "- [Storage command-line tool](../tutorials/storage/Storage%20command-line%20tool.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access Tables & Views on Google BigQuery\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "client = bigquery.Client(location=\"EU\")\n",
    "print(\"Client creating using default project: {}\".format(client.project))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explicitly specify a project when constructing the client, set the `project` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = bigquery.Client(location=\"US\", project=\"your-project-id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    SELECT `Set`, URL, Label\n",
    "    FROM `ecb-fsf-hackathon-base.hackathon_dataset.data_table`\n",
    "    LIMIT 60\n",
    "\"\"\"\n",
    "query_job = client.query(query, location=\"EU\")\n",
    "df = query_job.to_dataframe()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    SELECT category, url, product_name\n",
    "    FROM `ecb-fsf-hackathon-base.hackathon_dataset.web_scraped_data`\n",
    "    LIMIT 60\n",
    "\"\"\"\n",
    "query_job = client.query(query, location=\"EU\")\n",
    "df = query_job.to_dataframe()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also execute a query using the BigQuery magic expression in a cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery --verbose df\n",
    "SELECT category, Count(*) as Occurence\n",
    "FROM `ecb-fsf-hackathon-base.hackathon_dataset.web_scraped_data`\n",
    "GROUP BY category\n",
    "ORDER BY Occurence DESC\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learn more about interacting with BigQuery in the following tutorials:**\n",
    "- [BigQuery basics](../tutorials/bigquery/BigQuery%20basics.ipynb)\n",
    "- [BigQuery command-line tool](../tutorials/bigquery/BigQuery%20command-line%20tool.ipynb)\n",
    "- [BigQuery query magic](../tutorials/bigquery/BigQuery%20query%20magic.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud AI APIs and Cloud AutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful resources to get started with our Cloud APIs for NLP and [AutoML](https://cloud.google.com/automl/) for NLP:\n",
    "* [Cloud NLP Intro](https://cloud.google.com/natural-language/)\n",
    "* [Cloud Natural Language API Docs](https://cloud.google.com/natural-language/docs/)\n",
    "* [Cloud AutoML Get Started Guides](https://cloud.google.com/natural-language/overview/docs/get-started)\n",
    "* [Cloud AutoML NLP in the Console](https://console.cloud.google.com/natural-language)\n",
    "\n",
    "There is also [Cloud AutoML Tables](https://cloud.google.com/automl-tables/) to build ML models on tabular data (e.g. from BigQuery):\n",
    "* [Cloud AutoML Tables Intro](https://cloud.google.com/automl-tables/)\n",
    "* [Cloud AutoML Tables Docs](https://cloud.google.com/automl-tables/docs/)\n",
    "* [Cloud AutoML Tables in the Console](https://console.cloud.google.com/automl-tables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation with Apache Beam (and Cloud Dataflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install apache-beam[gcp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "pipeline_options = PipelineOptions.from_dictionary({\n",
    "    'runner': 'DirectRunner',\n",
    "# Run it massively parallel on Dataflow with\n",
    "#   'runner': 'DataflowRunner'\n",
    "    'job_name': 'notebook',\n",
    "    'streaming': True\n",
    "})\n",
    "\n",
    "def collect(i):\n",
    "    output.append(i)\n",
    "    return True\n",
    "\n",
    "output = []\n",
    "\n",
    "p = beam.Pipeline(options=pipeline_options)\n",
    "\n",
    "pipeline = (\n",
    "    p \n",
    "    | 'generate' >> beam.Create(range(1000))\n",
    "    | 'square' >> beam.Map(lambda x: x**2)\n",
    "    | \"print\" >> beam.Map(collect)\n",
    ")\n",
    "\n",
    "result = p.run()\n",
    "result.wait_until_finish()\n",
    "\n",
    "output[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models with Google Cloud AI Platform Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to enable the ML and Container Registry APIs in our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud services enable ml.googleapis.com\n",
    "!gcloud services enable containerregistry.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to create a bucket for the staging and training results. Replace with your favorite name (needs to be globally unique!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil mb gs://[YOUR_GCS_BUCKET]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ready to start our Training Job! Fill in in your bucket name where you find brackets. You can modify the model_dir parameter to change where the training output is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud ml-engine jobs submit training $JOB_NAME \\\n",
    "    --staging-bucket [YOUR_GCS_BUCKET] \\\n",
    "    --runtime-version 1.8 \\\n",
    "    --scale-tier BASIC_TPU \\\n",
    "    --module-name resnet.resnet_main \\\n",
    "    --package-path resnet/ \\\n",
    "    --region us-central1 \\\n",
    "    -- \\\n",
    "    --data_dir=gs://cloud-tpu-test-datasets/fake_imagenet \\\n",
    "    --model_dir=gs://[YOUR_GCS_BUCKET]/training_result/ \\\n",
    "    --resnet_depth=50 \\\n",
    "    --train_steps=1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn more about AI Platform Training & Serving with ML Engine:\n",
    "- [Training & Serving on ML Engine with SciKit Learn](../tutorials/cloud-ml-engine/Training%20and%20prediction%20with%20scikit-learn.ipynb)\n",
    "- [Github Repo full of Training & Prediction Examples](https://github.com/GoogleCloudPlatform/cloudml-samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visit the notebook [evaluation.ipynb](./evaluation.ipynb).**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
